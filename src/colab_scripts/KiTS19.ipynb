{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1863,"status":"ok","timestamp":1661008227764,"user":{"displayName":"Michael Yeung","userId":"03627978577428033999"},"user_tz":-60},"id":"PLq00TKThUG4","outputId":"605113f4-bfc0-4047-b622-4644e76565ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":[" #mount drive\n","from google.colab import drive\n","drive.mount('/content/drive',force_remount = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMGAWEzYhXNe"},"outputs":[],"source":["!pip install miscnn\n","!pip install matplotlib==3.3.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T0jaTYoShptt"},"outputs":[],"source":["# Standard libraries\n","import tensorflow as tf\n","import os\n","import random\n","import numpy as np\n","import argparse\n","import math\n","\n","# MIScnn\n","from miscnn.neural_network.metrics import identify_axis\n","from miscnn.processing.data_augmentation import Data_Augmentation\n","from miscnn.processing.subfunctions.normalization import Normalization\n","from miscnn.processing.subfunctions.resize import Resize\n","from miscnn.processing.subfunctions.clipping import Clipping\n","from miscnn.processing.subfunctions.resampling import Resampling\n","from miscnn.processing.preprocessor import Preprocessor\n","from miscnn.neural_network.model import Neural_Network\n","from miscnn.neural_network.metrics import dice_soft, dice_soft_loss\n","\n","from miscnn.neural_network.architecture.unet.standard import Architecture \n","\n","from miscnn.evaluation.cross_validation import run_fold, load_disk2fold\n","from miscnn.data_loading.interfaces.nifti_io import NIFTI_interface\n","from miscnn.data_loading.data_io import Data_IO\n","\n","# Tensorflow\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.callbacks import EarlyStopping"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14021,"status":"ok","timestamp":1661008247388,"user":{"displayName":"Michael Yeung","userId":"03627978577428033999"},"user_tz":-60},"id":"IueHGM6OiJ_9","outputId":"93c4c547-9210-49cf-ca72-3f0ed6219886"},"outputs":[{"name":"stdout","output_type":"stream","text":["All samples: 300\n","Image dimension check: (611, 512, 512, 1) (611, 512, 512, 1)\n"]}],"source":["#Generating interface: single channel with 3 classes\n","interface = NIFTI_interface(pattern = \"case_[0-9]*\", channels=1, classes=3)\n","\n","# path to data folder\n","data_path =\"/content/drive/My Drive/KITS19/data\"\n","\n","# Generating dataloader\n","data_io = Data_IO(interface, data_path, delete_batchDir=False)\n","\n","# Sample list\n","sample_list = data_io.get_indiceslist()\n","sample_list.sort()\n","\n","# Basic checks\n","print(\"All samples: \" + str(len(sample_list)))\n","sample = data_io.sample_loader(sample_list[0], load_seg=True)  \n","print(\"Image dimension check:\",sample.img_data.shape, sample.seg_data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2nbwQg4wisn1"},"outputs":[],"source":["# Data augmentation\n","data_aug = Data_Augmentation(cycles=1, scaling=True, rotations=True, elastic_deform=True, mirror=True,\n","                             brightness=True, contrast=False, gamma=False, gaussian_noise=False)\n","\n","\n","# Data preprocessing parameters\n","sf_normalize = Normalization(mode='z-score')\n","sf_clipping = Clipping(min=-79, max=304)\n","sf_resample = Resampling((3.22, 1.62, 1.62))\n","subfunctions = [sf_resample, sf_clipping, sf_normalize]\n","\n","\n","# Create preprocessing class\n","pp = Preprocessor(data_io\n","                  , data_aug=data_aug\n","                  , batch_size=1\n","                  , prepare_subfunctions=True\n","                  , subfunctions=subfunctions\n","                  , prepare_batches=False\n","                  , analysis=\"patchwise-crop\"\n","                  , patch_shape=(80, 160, 160)\n","                  , use_multiprocessing=False\n","                  )\n","\n","# Adjust the patch overlap for predictions\n","pp.patchwise_overlap = (40, 80, 80)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lKfOsILHjCwg"},"outputs":[],"source":["# Loss functions\n","\n","# Dice++ loss\n","def dice_plus_loss(gamma=2):\n","\n","    def loss_function(y_true, y_pred):\n","        axis=identify_axis(y_true.get_shape())\n","        epsilon = K.epsilon()\n","        y_pred = K.clip(y_pred,epsilon,1-epsilon)\n","        y_true = K.clip(y_true,epsilon,1-epsilon)\n","        delta = 0.5\n","        tp = K.sum(y_true * y_pred, axis=axis)\n","        fn = K.sum((y_true * (1-y_pred))**gamma, axis=axis)\n","        fp = K.sum(((1-y_true) * y_pred)**gamma, axis=axis)\n","        dice_class = (tp + epsilon)/(tp + delta*fn + (1-delta)*fp + epsilon)\n","        loss = K.mean(1-dice_class)\n","\n","        return loss\n","\n","    return loss_function\n","\n","# Cross entropy loss\n","def cross_entropy(y_true, y_pred):\n","\n","        axis = identify_axis(y_true.get_shape())\n","        epsilon = K.epsilon()\n","        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n","        cross_entropy = -y_true * K.log(y_pred)\n","        cross_entropy = K.mean(K.sum(cross_entropy, axis=[-1]))\n","\n","        return cross_entropy\n","\n","# Focal loss\n","def focal_loss(alpha=0.5, gamma=2.):\n","    def loss_function(y_true, y_pred):\n","        axis = identify_axis(y_true.get_shape())\n","        # Clip values to prevent division by zero error\n","        epsilon = K.epsilon()\n","        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n","        cross_entropy = -y_true * K.log(y_pred)\n","\n","        if alpha is not None:\n","            alpha_weight = np.array(alpha, dtype=np.float32)\n","            focal_loss = alpha_weight * K.pow(1 - y_pred, gamma) * cross_entropy\n","        else:\n","            focal_loss = K.pow(1 - y_pred, gamma) * cross_entropy\n","\n","        focal_loss = K.mean(K.sum(focal_loss, axis=[-1]))\n","        return focal_loss\n","        \n","    return loss_function\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jSJXcS-iyA1"},"outputs":[],"source":["# U-Net\n","architecture = Architecture()\n","\n","# Here the loss function is chosen:\n","# Either cross_entropy, dice_soft_loss, focal_loss() or dice_plus_loss()\n","loss = dice_plus_loss()\n","\n","# Create the Neural Network model\n","model = Neural_Network(architecture=architecture\n","                      , preprocessor=pp\n","                      , loss=loss\n","                      , metrics=[dice_soft]\n","                      , batch_queue_size=3\n","                      , workers=1\n","                      , learning_rate=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UOvr0qljglu"},"outputs":[],"source":["# Learning rate schedule\n","cb_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='min', min_delta=1e-7, cooldown=1,    \n","                          min_lr=1e-4)\n","\n","cb_es = EarlyStopping(monitor='val_loss', min_delta=1e-7, patience=10, verbose=1, mode='min')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWZQ_SWijoJL"},"outputs":[],"source":["# Run pipeline for cross-validation fold\n","run_fold(fold=0\n","         , model=model\n","         , epochs=1000\n","         , evaluation_path='/content/drive/My Drive/KITS19/output'\n","         , draw_figures=True\n","         , callbacks=[cb_lr,cb_es]\n","         , save_models=True\n","        )"]},{"cell_type":"markdown","source":["**Evaluation code**"],"metadata":{"id":"qpc1tCmGPmUr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2pbD58okRUi"},"outputs":[],"source":["from miscnn.data_loading.data_io import backup_evaluation\n","import json\n","import skimage\n","from skimage.transform import resize\n","\n","path = os.path.join('/content/drive/My Drive/KITS19/output','fold_0','model.hdf5')\n","training,validation = load_disk2fold(os.path.join('/content/drive/My Drive/KITS19/output','fold_0','testing.json'))\n","evaluation_path = os.path.join('/content/drive/My Drive/KITS19/output/fold_0')\n","\n","model.load(path)\n","\n","def load_disk2fold(file_path):\n","    with open(file_path, \"r\") as jsonfile:\n","        sampling = json.load(jsonfile)\n","        if \"TRAINING\" in sampling : training = sampling[\"TRAINING\"]\n","        else : training = None\n","        if \"VALIDATION\" in sampling : validation = sampling[\"VALIDATION\"]\n","        else : validation = None\n","    return training, validation\n","\n","\n","def compute_brier(truth, pred, classes):\n","    brier_scores_1 = []\n","    brier_scores_2 = []\n","    # Compute Dice for each class\n","    i = 1\n","    try:\n","        brier = np.mean(((truth[:,:,:,0] - pred[:,:,:,i])**2))\n","        brier_scores_1.append(brier)\n","    except ZeroDivisionError:\n","        brier_scores_1.append(0.0)\n","\n","    i = 2\n","    try:\n","        brier = np.mean(((truth[:,:,:,0] - pred[:,:,:,i])**2))\n","        brier_scores_2.append(brier)\n","    except ZeroDivisionError:\n","        brier_scores_2.append(0.0)\n","    # Return computed Dice scores\n","    return brier_scores_1, brier_scores_2\n","\n","def compute_nll(truth, pred, classes):\n","    nll_scores_1 = []\n","    nll_scores_2 = []\n","    # Compute Dice for each class\n","    i = 1\n","    try:\n","        epsilon = K.epsilon()   \n","        pred = np.clip(pred, epsilon, 1. - epsilon)\n","        cross_entropy = -(truth[:,:,:,0] * np.log(pred[:,:,:,i]))\n","\n","        cross_entropy = np.mean(cross_entropy)\n","        nll_scores_1.append(cross_entropy)\n","\n","    except ZeroDivisionError:\n","        nll_scores_1.append(0.0)\n","\n","    i = 2\n","    try:\n","        epsilon = K.epsilon()   \n","        pred = np.clip(pred, epsilon, 1. - epsilon)\n","        cross_entropy = -(truth[:,:,:,0] * np.log(pred[:,:,:,i]))\n","\n","        cross_entropy = np.mean(cross_entropy)\n","        nll_scores_2.append(cross_entropy)\n","\n","    except ZeroDivisionError:\n","        nll_scores_2.append(0.0)\n","    # Return computed Dice scores\n","    return nll_scores_1, nll_scores_2\n","\n","# Calculate class-wise dice similarity coefficient\n","def compute_dice(truth, pred, classes):\n","    dice_scores_1 = []\n","    dice_scores_2 = []\n","    # Compute Dice for each class\n","    i = 1\n","    try:\n","        pd = np.equal(pred, i)\n","        gt = np.equal(truth[:,:,:,0], i)\n","        dice = 2*np.logical_and(pd, gt).sum()/(pd.sum() + gt.sum())\n","        dice_scores_1.append(dice)\n","    except ZeroDivisionError:\n","        dice_scores_1.append(0.0)\n","\n","    i = 2\n","    try:\n","        pd = np.equal(pred, i)\n","        gt = np.equal(truth[:,:,:,0], i)\n","        dice = 2*np.logical_and(pd, gt).sum()/(pd.sum() + gt.sum())\n","        dice_scores_2.append(dice)\n","    except ZeroDivisionError:\n","        dice_scores_2.append(0.0)\n","    # Return computed Dice scores\n","    return dice_scores_1, dice_scores_2\n","\n","def compute_iou(truth, pred, classes):\n","    iou_scores_1 = []\n","    iou_scores_2 = []\n","    # Compute Dice for each class\n","    i = 1\n","    try:\n","        pd = np.equal(pred, i)\n","        gt = np.equal(truth[:,:,:,0], i)\n","        tp = np.logical_and(pd,gt).sum()\n","        fp = np.logical_and(pd,np.logical_not(gt)).sum()\n","        fn = np.logical_and(gt,np.logical_not(pd)).sum()\n","        iou = tp / (tp + fp + fn)\n","        iou_scores_1.append(iou)\n","    except ZeroDivisionError:\n","        iou_scores_1.append(0.0)\n","\n","    i = 2\n","    try:\n","        pd = np.equal(pred, i)\n","        gt = np.equal(truth[:,:,:,0], i)\n","        tp = np.logical_and(pd,gt).sum()\n","        fp = np.logical_and(pd,np.logical_not(gt)).sum()\n","        fn = np.logical_and(gt,np.logical_not(pd)).sum()\n","        iou = tp / (tp + fp + fn)\n","        iou_scores_2.append(iou)\n","    except ZeroDivisionError:\n","        iou_scores_2.append(0.0)\n","\n","    return iou_scores_1, iou_scores_2\n","\n","def compute_rest(truth, pred, classes):\n","    precision_scores_1 = []\n","    recall_scores_1 = []\n","    precision_scores_2 = []\n","    recall_scores_2 = []\n","    # Compute precision, recall scores for each class\n","    i = 1\n","    try:\n","        pd = np.equal(pred, i)\n","        gt = np.equal(truth[:,:,:,0], i)\n","        tp = np.logical_and(pd,gt).sum()\n","        fp = np.logical_and(pd,np.logical_not(gt)).sum()\n","        fn = np.logical_and(gt,np.logical_not(pd)).sum()\n","        precision = tp/(tp+fp)\n","        precision_scores_1.append(precision)\n","    except ZeroDivisionError:\n","        precision_scores_1.append(0.0) \n","\n","    try:\n","        recall = tp/(tp+fn)\n","        recall_scores_1.append(recall)\n","    except ZeroDivisionError:\n","        recall_scores_1.append(0.0)\n","\n","    i = 2\n","    try:\n","        pd = np.equal(pred, i)\n","        gt = np.equal(truth[:,:,:,0], i)\n","        tp = np.logical_and(pd,gt).sum()\n","        fp = np.logical_and(pd,np.logical_not(gt)).sum()\n","        fn = np.logical_and(gt,np.logical_not(pd)).sum()\n","        precision = tp/(tp+fp)\n","        precision_scores_2.append(precision)\n","    except ZeroDivisionError:\n","        precision_scores_2.append(0.0) \n","\n","    try:\n","        recall = tp/(tp+fn)\n","        recall_scores_2.append(recall)\n","    except ZeroDivisionError:\n","        recall_scores_2.append(0.0)\n","            \n","    # Return computed precision, recall scores \n","    return precision_scores_1, recall_scores_1, precision_scores_2, recall_scores_2\n","\n","# Initialize detailed validation scoring file\n","classes = [\"nll_score_1\"]\n","classes_1 = [\"brier_score_1\"]\n","classes_2 = [\"dice_score_1\"]\n","classes_3 = [\"iou_score_1\"]  \n","classes_4 = [\"precision_score_1\"]  \n","classes_5 = [\"recall_score_1\"]    \n","classes_6 = [\"nll_score_2\"]\n","classes_7 = [\"brier_score_2\"]\n","classes_8 = [\"dice_score_2\"]\n","classes_9 = [\"iou_score_2\"]  \n","classes_10 = [\"precision_score_2\"]  \n","classes_11 = [\"recall_score_2\"]           \n","header = [\"sample_id\"]\n","header.extend(classes)\n","header.extend(classes_1)\n","header.extend(classes_2)\n","header.extend(classes_3)\n","header.extend(classes_4)\n","header.extend(classes_5)\n","header.extend(classes_6)\n","header.extend(classes_7)\n","header.extend(classes_8)\n","header.extend(classes_9)\n","header.extend(classes_10)\n","header.extend(classes_11)\n","backup_evaluation(header, evaluation_path, start=True)\n","\n","\n","for sample_index in validation:\n","        pred = model.predict([sample_index],return_output=True,activation_output=True)\n","\n","\n","        pred = pred[0]\n","\n","        # Load the sample\n","        sample = model.preprocessor.data_io.sample_loader(sample_index,\n","                                                          load_seg=True,\n","                                                          load_pred=False)\n","        # Access image, truth and predicted segmentation data\n","        img, seg = sample.img_data, sample.seg_data\n","\n","        # resize segmentation to size of prediction activations\n","        seg_act = resize(seg,(pred.shape[0], pred.shape[1], pred.shape[2]),order=0, preserve_range=True,anti_aliasing=False)\n","\n","        # Compute calibration metrics\n","        nll_scores_1, nll_scores_2 = compute_nll(seg_act, pred, 1)\n","        brier_scores_1, brier_scores_2 = compute_brier(seg_act, pred, 1)\n","\n","        # convert softmax predictions to classes\n","        pred = np.argmax(pred,axis=-1)\n","\n","         # resize final prediction to original image shape\n","        pred = resize(pred,(seg.shape[0], seg.shape[1], seg.shape[2]),order=0, preserve_range=True,anti_aliasing=False)\n","\n","        # Compute segmentation metrics\n","        dice_scores_1, dice_scores_2 = compute_dice(seg, pred, 1)\n","        iou_scores_1, iou_scores_2 = compute_iou(seg, pred, 1)\n","        precision_scores_1, recall_scores_1, precision_scores_2, recall_scores_2 = compute_rest(seg, pred, 1)\n","\n","        # Save detailed validation scores to file\n","        scores = [sample_index]\n","        scores.extend(nll_scores_1)\n","        scores.extend(brier_scores_1)\n","        scores.extend(dice_scores_1)\n","        scores.extend(iou_scores_1)\n","        scores.extend(precision_scores_1)\n","        scores.extend(recall_scores_1)\n","        scores.extend(nll_scores_2)\n","        scores.extend(brier_scores_2)\n","        scores.extend(dice_scores_2)\n","        scores.extend(iou_scores_2)\n","        scores.extend(precision_scores_2)\n","        scores.extend(recall_scores_2)\n","        backup_evaluation(scores, evaluation_path, start=False)   "]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"KiTS19.ipynb","provenance":[],"authorship_tag":"ABX9TyNnmDu9gK4fWio5C6ZQjcJH"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}