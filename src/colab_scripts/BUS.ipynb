{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BUS.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM4PwjjxBfPAXQwg861Bp7Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PLq00TKThUG4","executionInfo":{"status":"ok","timestamp":1661004043078,"user_tz":-60,"elapsed":1656,"user":{"displayName":"Michael Yeung","userId":"03627978577428033999"}},"outputId":"3b6143a9-3934-4128-b2ba-3af55713e4f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["#mount drive\n","from google.colab import drive\n","drive.mount('/content/drive',force_remount = False)"]},{"cell_type":"code","source":["!pip install miscnn\n","!pip install matplotlib==3.3.1"],"metadata":{"id":"oMGAWEzYhXNe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Training code**"],"metadata":{"id":"224eWCfCMmAt"}},{"cell_type":"code","source":["# Standard libraries\n","import tensorflow as tf\n","import os\n","import random\n","import numpy as np\n","import argparse\n","import math\n","\n","# MIScnn\n","from miscnn.neural_network.metrics import identify_axis\n","from miscnn.processing.data_augmentation import Data_Augmentation\n","from miscnn.processing.subfunctions.normalization import Normalization\n","from miscnn.processing.subfunctions.resize import Resize\n","from miscnn.processing.subfunctions.clipping import Clipping\n","from miscnn.processing.subfunctions.resampling import Resampling\n","from miscnn.processing.preprocessor import Preprocessor\n","from miscnn.neural_network.model import Neural_Network\n","from miscnn.neural_network.metrics import dice_soft, dice_soft_loss\n","\n","from miscnn.neural_network.architecture.unet.standard import Architecture \n","\n","from miscnn.evaluation.cross_validation import run_fold, load_disk2fold\n","from miscnn.data_loading.interfaces.image_io import Image_interface\n","from miscnn.data_loading.data_io import Data_IO\n","\n","# Tensorflow\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.callbacks import EarlyStopping"],"metadata":{"id":"T0jaTYoShptt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Generating interface: single channel with 2 classes\n","interface = Image_interface(pattern = \"case_[0-9]*\" # Pattern may differ depending on folder structure\n","                                 , img_type = 'rgb'\n","                                 , img_format = 'png'\n","                                 , classes = 2)\n","\n","# path to data folder\n","data_path = \"/content/drive/My Drive/BUS2017/data\" # Path to DRIVE dataset\n","\n","# Generating dataloader\n","data_io = Data_IO(interface, data_path, delete_batchDir=False)\n","\n","# Sample list\n","sample_list = data_io.get_indiceslist()\n","sample_list.sort()\n","\n","# Basic checks\n","print(\"All samples: \" + str(len(sample_list)))\n","sample = data_io.sample_loader(sample_list[0], load_seg=True)  \n","print(\"Image dimension check:\",sample.img_data.shape, sample.seg_data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IueHGM6OiJ_9","executionInfo":{"status":"ok","timestamp":1661004064516,"user_tz":-60,"elapsed":961,"user":{"displayName":"Michael Yeung","userId":"03627978577428033999"}},"outputId":"3680b9b2-410f-4d72-bc6f-4054e719ff7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["All samples: 163\n","Image dimension check: (360, 528, 3) (360, 528, 1)\n"]}]},{"cell_type":"code","source":["# Data augmentation\n","data_aug = Data_Augmentation(cycles=1, scaling=True, rotations=True, elastic_deform=True, mirror=True,\n","                             brightness=True, contrast=False, gamma=False, gaussian_noise=False)\n","\n","\n","# Data preprocessing parameters\n","sf_normalize = Normalization(mode='z-score')\n","sf_resize = Resize((128,128))\n","subfunctions = [sf_resize, sf_normalize]\n","\n","\n","# Create preprocessing class\n","pp = Preprocessor(data_io\n","                  , data_aug=data_aug\n","                  , batch_size=1\n","                  , prepare_subfunctions=True\n","                  , subfunctions=subfunctions\n","                  , prepare_batches=False\n","                  , analysis=\"fullimage\"\n","                  , use_multiprocessing=False)"],"metadata":{"id":"2nbwQg4wisn1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loss functions\n","\n","# Dice++ loss\n","def dice_plus_loss(gamma=2):\n","\n","    def loss_function(y_true, y_pred):\n","        axis=identify_axis(y_true.get_shape())\n","        epsilon = K.epsilon()\n","        y_pred = K.clip(y_pred,epsilon,1-epsilon)\n","        y_true = K.clip(y_true,epsilon,1-epsilon)\n","        delta = 0.5\n","        tp = K.sum(y_true * y_pred, axis=axis)\n","        fn = K.sum((y_true * (1-y_pred))**gamma, axis=axis)\n","        fp = K.sum(((1-y_true) * y_pred)**gamma, axis=axis)\n","        dice_class = (tp + epsilon)/(tp + delta*fn + (1-delta)*fp + epsilon)\n","        loss = K.mean(1-dice_class)\n","\n","        return loss\n","\n","    return loss_function\n","\n","# Cross entropy loss\n","def cross_entropy(y_true, y_pred):\n","\n","        axis = identify_axis(y_true.get_shape())\n","        epsilon = K.epsilon()\n","        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n","        cross_entropy = -y_true * K.log(y_pred)\n","        cross_entropy = K.mean(K.sum(cross_entropy, axis=[-1]))\n","\n","        return cross_entropy\n","\n","# Focal loss\n","def focal_loss(alpha=0.5, gamma=2.):\n","    def loss_function(y_true, y_pred):\n","        axis = identify_axis(y_true.get_shape())\n","        # Clip values to prevent division by zero error\n","        epsilon = K.epsilon()\n","        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n","        cross_entropy = -y_true * K.log(y_pred)\n","\n","        if alpha is not None:\n","            alpha_weight = np.array(alpha, dtype=np.float32)\n","            focal_loss = alpha_weight * K.pow(1 - y_pred, gamma) * cross_entropy\n","        else:\n","            focal_loss = K.pow(1 - y_pred, gamma) * cross_entropy\n","\n","        focal_loss = K.mean(K.sum(focal_loss, axis=[-1]))\n","        return focal_loss\n","        \n","    return loss_function\n"],"metadata":{"id":"lKfOsILHjCwg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# U-Net\n","architecture = Architecture()\n","\n","# Here the loss function is chosen:\n","# Either cross_entropy, dice_soft_loss, focal_loss() or dice_plus_loss()\n","loss = dice_plus_loss()\n","\n","# Create the Neural Network model\n","model = Neural_Network(architecture=architecture\n","                      , preprocessor=pp\n","                      , loss=loss\n","                      , metrics=[dice_soft]\n","                      , batch_queue_size=3\n","                      , workers=1\n","                      , learning_rate=0.1)\n"],"metadata":{"id":"6jSJXcS-iyA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Learning rate schedule\n","cb_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=25, verbose=1, mode='min', min_delta=1e-7, cooldown=1,    \n","                          min_lr=1e-4)\n","\n","cb_es = EarlyStopping(monitor='val_loss', min_delta=1e-7, patience=50, verbose=1, mode='min')"],"metadata":{"id":"8UOvr0qljglu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run pipeline for cross-validation fold\n","run_fold(fold=0\n","         , model=model\n","         , epochs=1000\n","         , evaluation_path='/content/drive/My Drive/BUS2017/output'\n","         , draw_figures=True\n","         , callbacks=[cb_lr,cb_es]\n","         , save_models=True\n","        )"],"metadata":{"id":"bWZQ_SWijoJL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Evaluation code**"],"metadata":{"id":"b3EC2UMiNFzl"}},{"cell_type":"code","source":["from miscnn.data_loading.data_io import backup_evaluation\n","import json\n","import skimage\n","from skimage.transform import resize\n","import matplotlib.pyplot as plt\n","\n","# Change paths as necessary\n","path = os.path.join('/content/drive/My Drive/BUS2017/output','fold_0','model.hdf5')\n","training,validation = load_disk2fold(os.path.join('/content/drive/My Drive/BUS2017/output','fold_0','testing.json'))\n","evaluation_path = os.path.join('/content/drive/My Drive/BUS2017/output/fold_0')\n","\n","# load model\n","model.load(path)\n","\n","\n","def load_disk2fold(file_path):\n","    with open(file_path, \"r\") as jsonfile:\n","        sampling = json.load(jsonfile)\n","        if \"TRAINING\" in sampling : training = sampling[\"TRAINING\"]\n","        else : training = None\n","        if \"VALIDATION\" in sampling : validation = sampling[\"VALIDATION\"]\n","        else : validation = None\n","    return training, validation\n","\n","\n","def compute_brier(truth, pred, classes):\n","    brier_scores = []\n","    # Compute Dice for each class\n","    i = 1\n","    try:\n","        brier = np.mean(((truth[:,:,0] - pred[:,:,classes])**2))\n","        brier_scores.append(brier)\n","    except ZeroDivisionError:\n","        brier_scores.append(0.0)\n","    # Return computed Dice scores\n","    return brier_scores\n","\n","def compute_nll(truth, pred, classes):\n","    nll_scores = []\n","    # Compute Dice for each class\n","    i = 1\n","    try:\n","        epsilon = K.epsilon()   \n","        pred = np.clip(pred, epsilon, 1. - epsilon)\n","        cross_entropy = -(truth[:,:,0] * np.log(pred[:,:,classes]))\n","\n","        cross_entropy = np.mean(cross_entropy)\n","        nll_scores.append(cross_entropy)\n","\n","    except ZeroDivisionError:\n","        nll_scores.append(0.0)\n","    # Return computed Dice scores\n","    return nll_scores\n","\n","# Calculate class-wise dice similarity coefficient\n","def compute_dice(truth, pred, classes):\n","    dice_scores = []\n","    # Compute Dice for each class\n","    i = 1\n","    try:\n","        pd = np.equal(pred, i)\n","        gt = np.equal(truth[:,:,0], i)\n","        dice = 2*np.logical_and(pd, gt).sum()/(pd.sum() + gt.sum())\n","        dice_scores.append(dice)\n","    except ZeroDivisionError:\n","        dice_scores.append(0.0)\n","    # Return computed Dice scores\n","    return dice_scores\n","\n","def compute_iou(truth, pred, classes):\n","    iou_scores = []\n","    # Compute Dice for each class\n","    i = 1\n","    try:\n","        pd = np.equal(pred, i)\n","        gt = np.equal(truth[:,:,0], i)\n","        tp = np.logical_and(pd,gt).sum()\n","        fp = np.logical_and(pd,np.logical_not(gt)).sum()\n","        fn = np.logical_and(gt,np.logical_not(pd)).sum()\n","        iou = tp / (tp + fp + fn)\n","        iou_scores.append(iou)\n","    except ZeroDivisionError:\n","        iou_scores.append(0.0)\n","    return iou_scores\n","\n","def compute_rest(truth, pred, classes):\n","    precision_scores = []\n","    recall_scores = []\n","    # Compute precision, recall scores for each class\n","    i = 1\n","    try:\n","        pd = np.equal(pred, i)\n","        gt = np.equal(truth[:,:,0], i)\n","        tp = np.logical_and(pd,gt).sum()\n","        fp = np.logical_and(pd,np.logical_not(gt)).sum()\n","        fn = np.logical_and(gt,np.logical_not(pd)).sum()\n","        precision = tp/(tp+fp)\n","        precision_scores.append(precision)\n","    except ZeroDivisionError:\n","        precision_scores.append(0.0) \n","\n","    try:\n","        recall = tp/(tp+fn)\n","        recall_scores.append(recall)\n","    except ZeroDivisionError:\n","        recall_scores.append(0.0)\n","            \n","    # Return computed precision, recall scores \n","    return precision_scores, recall_scores\n","\n","# Initialize detailed validation scoring file\n","classes = [\"nll_score\"]\n","classes_1 = [\"brier_score\"]\n","classes_2 = [\"dice_score\"]\n","classes_3 = [\"iou_score\"]  \n","classes_4 = [\"precision_score\"]  \n","classes_5 = [\"recall_score\"]          \n","header = [\"sample_id\"]\n","header.extend(classes)\n","header.extend(classes_1)\n","header.extend(classes_2)\n","header.extend(classes_3)\n","header.extend(classes_4)\n","header.extend(classes_5)\n","backup_evaluation(header, evaluation_path, start=True)\n","\n","for sample_index in validation:\n","        pred = model.predict([sample_index],return_output=True,activation_output=True)\n","\n","        # get prediction from list of predictions\n","        pred = pred[0]\n","\n","        # Load the sample\n","        sample = model.preprocessor.data_io.sample_loader(sample_index,\n","                                                          load_seg=True,\n","                                                          load_pred=False)\n","        # Access image and ground truth segmentation data\n","        img, seg = sample.img_data, sample.seg_data\n","\n","        # resize segmentation to size of prediction activations\n","        seg_act = resize(seg,(pred.shape[0], pred.shape[1]),order=0, preserve_range=True,anti_aliasing=False)\n","\n","        # Compute calibration metrics\n","        nll_scores = compute_nll(seg_act, pred, 1)\n","        brier_scores = compute_brier(seg_act, pred, 1)\n","\n","        # convert softmax predictions to classes\n","        pred = np.argmax(pred,axis=-1)\n","        \n","        # resize final prediction to original image shape\n","        pred = resize(pred,(seg.shape[0], seg.shape[1]),order=0, preserve_range=True,anti_aliasing=False)\n","\n","        # Compute segmentation metrics\n","        dice_scores = compute_dice(seg, pred, 1)\n","        iou_scores = compute_iou(seg, pred, 1)\n","        precision_scores, recall_scores = compute_rest(seg, pred, 1)\n","        \n","        # Save detailed validation scores to file\n","        scores = [sample_index]\n","        scores.extend(nll_scores)\n","        scores.extend(brier_scores)\n","        scores.extend(dice_scores)\n","        scores.extend(iou_scores)\n","        scores.extend(precision_scores)\n","        scores.extend(recall_scores)\n","        backup_evaluation(scores, evaluation_path, start=False)   "],"metadata":{"id":"m2pbD58okRUi"},"execution_count":null,"outputs":[]}]}